{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kCfH3wcOZnMdRdlhLdQAFAFl6AzAv8wF",
      "authorship_tag": "ABX9TyMCwlj7zvCSdI6JT1upsP7H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qR4V3qX5Nzlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56cfc21-0898-4644-d3da-97aa8512f325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 43.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "from transformers import get_scheduler\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "a_UeJAGsN_FR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self, train_path, valid_path, batch_size, num_workers, epochs, lr, shuffle, gpu_num, vocab_rev):\n",
        "        self.train_path = train_path\n",
        "        self.valid_path = valid_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.shuffle = shuffle\n",
        "        self.device = torch.device(f'cuda:{gpu_num}' if torch.cuda.is_available() else 'cpu')\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.vocab_rev = vocab_rev\n",
        "\n",
        "args = Args('/content/drive/MyDrive/Data/archive/articles1.csv',\n",
        "            '/content/drive/MyDrive/Data/archive/articles2.csv',\n",
        "            2,\n",
        "            0,\n",
        "            30,\n",
        "            7e-4,\n",
        "            True,\n",
        "            0,\n",
        "            False)\n",
        "args.train_path, args.device"
      ],
      "metadata": {
        "id": "lcogVAdpT1EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc75e867-dde8-40b6-c6d0-7a3f08d17fa1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Data/archive/articles1.csv', device(type='cpu'))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "# lm_head.weight \t torch.Size([50257, 768]) 추가\n",
        "# loss func.이 내재되어 있음\n",
        "\n",
        "want_to_change_vocab = args.vocab_rev\n",
        "pad_id = 50267 if want_to_change_vocab else tokenizer.eos_token_id\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=pad_id) "
      ],
      "metadata": {
        "id": "8-sHWuBmODTp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "input_ids = tokenizer.encode('''Zelensky also reiterated his appeal for a NATO no-fly zone over Ukraine,\n",
        " saying \"an immediate closure of the skies over Ukraine is needed.\"''', return_tensors='pt')\n",
        "\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=50, \n",
        "    top_p=0.45, \n",
        "    num_return_sequences=3 )\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9xEWo2GqxGi",
        "outputId": "24b9219a-e324-4f2b-a8e7-c66762f8268e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Zelensky also reiterated his appeal for a NATO no-fly zone over Ukraine, saying \"an immediate closure of the skies over Ukraine is needed.\"\n",
            "\n",
            "\"The international community must now act on the need to ensure that Ukraine's airspace is not used for terrorist activities,\" he said.\n",
            "\n",
            "The UN Security Council on Monday unanimously voted to send the Security Council to vote on the resolution, which was adopted in the Security Council on May 17.\n",
            "\n",
            "The Security Council resolution is a response\n",
            "1: Zelensky also reiterated his appeal for a NATO no-fly zone over Ukraine, saying \"an immediate closure of the skies over Ukraine is needed.\"\n",
            "\n",
            "\"The Russian military is ready to defend itself against any threat,\" he said.\n",
            "\n",
            "NATO's military alliance has been struggling to contain the growing violence in eastern Ukraine since Kiev seized control of the city of Donetsk on March 8.\n",
            "\n",
            "\"The situation in Ukraine is not good,\" said NATO Secretary-General Jens Stol\n",
            "2: Zelensky also reiterated his appeal for a NATO no-fly zone over Ukraine, saying \"an immediate closure of the skies over Ukraine is needed.\"\n",
            "\n",
            "\"It is a matter of urgency that we have to get the situation under control and to make sure that no civilians are killed or injured in any way,\" he said.\n",
            "\n",
            "NATO Secretary General Jens Stoltenberg said in a statement that \"we have made clear that we are not going to accept any unilateral military action\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### test\n",
        "for layer in model.state_dict():\n",
        "    print(layer, '\\t', model.state_dict()[layer].size())"
      ],
      "metadata": {
        "id": "yBFj7glUHYl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t_7 training, q_7, k_7, v_7\"만\" 계산하고\n",
        "# t_6까지 계산한 k_1 ... k_6 또는 v_1 ... v_6 재탕\n",
        "for i, block in enumerate(output.past_key_values):\n",
        "    key, value = block\n",
        "    print(f'attn {i+1} : ', key.size())\n",
        "    print(f'attn {i+1} : ', value.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq2USftsPDiQ",
        "outputId": "70c8cdf3-9e66-471b-960e-e8d0885854b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn 1 :  torch.Size([1, 12, 10, 64])\n",
            "attn 1 :  torch.Size([1, 12, 10, 64])\n",
            "attn 2 :  torch.Size([1, 12, 10, 64])\n",
            "attn 2 :  torch.Size([1, 12, 10, 64])\n",
            "attn 3 :  torch.Size([1, 12, 10, 64])\n",
            "attn 3 :  torch.Size([1, 12, 10, 64])\n",
            "attn 4 :  torch.Size([1, 12, 10, 64])\n",
            "attn 4 :  torch.Size([1, 12, 10, 64])\n",
            "attn 5 :  torch.Size([1, 12, 10, 64])\n",
            "attn 5 :  torch.Size([1, 12, 10, 64])\n",
            "attn 6 :  torch.Size([1, 12, 10, 64])\n",
            "attn 6 :  torch.Size([1, 12, 10, 64])\n",
            "attn 7 :  torch.Size([1, 12, 10, 64])\n",
            "attn 7 :  torch.Size([1, 12, 10, 64])\n",
            "attn 8 :  torch.Size([1, 12, 10, 64])\n",
            "attn 8 :  torch.Size([1, 12, 10, 64])\n",
            "attn 9 :  torch.Size([1, 12, 10, 64])\n",
            "attn 9 :  torch.Size([1, 12, 10, 64])\n",
            "attn 10 :  torch.Size([1, 12, 10, 64])\n",
            "attn 10 :  torch.Size([1, 12, 10, 64])\n",
            "attn 11 :  torch.Size([1, 12, 10, 64])\n",
            "attn 11 :  torch.Size([1, 12, 10, 64])\n",
            "attn 12 :  torch.Size([1, 12, 10, 64])\n",
            "attn 12 :  torch.Size([1, 12, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### test\n",
        "train_path = '/content/drive/MyDrive/Data/archive/articles1.csv'\n",
        "data = pd.read_csv(train_path, index_col=0)[['content']]\n",
        "safe_len = data['content'].apply(lambda x: len(x.split(' ')) < 350)\n",
        "print(safe_len.value_counts())\n",
        "\n",
        "clean = []\n",
        "for line in data['content']:\n",
        "    if len(line.split(' ')) > 350:\n",
        "        truncated_line = ' '.join(line.split(' ')[:350])\n",
        "        clean.append(truncated_line.lower())\n",
        "clean = pd.DataFrame(clean, columns=['content'])\n",
        "clean['content'].apply(lambda x: len(x.split(' ')) < 350).value_counts()\n",
        "pd.concat([data[safe_len], clean], axis=0).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "MAncMeY6vRTY",
        "outputId": "a3c0100e-85e7-41d4-ea94-914e726f601e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False    33982\n",
            "True     16018\n",
            "Name: content, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-349784bb-5261-4997-a5d4-47eb65b68dfe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LONDON  —   Queen Elizabeth II, who has been b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SYDNEY, Australia  —   The annual beach pilgri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>The nation’s consumer watchdog agency on Tuesd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>It was a close call for the queen. A walk arou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>Nearly 200 of the approximately 450 people who...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-349784bb-5261-4997-a5d4-47eb65b68dfe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-349784bb-5261-4997-a5d4-47eb65b68dfe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-349784bb-5261-4997-a5d4-47eb65b68dfe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               content\n",
              "5    LONDON  —   Queen Elizabeth II, who has been b...\n",
              "19   SYDNEY, Australia  —   The annual beach pilgri...\n",
              "79   The nation’s consumer watchdog agency on Tuesd...\n",
              "106  It was a close call for the queen. A walk arou...\n",
              "107  Nearly 200 of the approximately 450 people who..."
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 애초에 attn mask가 있기 때문에 안해도 무방\n",
        "# 그러나 open-end generation을 피하는 한 가지 방법\n",
        "\n",
        "# '<|endoftext|>': 50256\n",
        "# 마구 추가하면 pretrain 방식과 다르기 때문에 성능을 저하시킬 수 있다\n",
        "if want_to_change_vocab:\n",
        "    special_tokens =  {'pad_token': '[PAD]'}\n",
        "                    # 'bos_token': '<|endoftext|>', \n",
        "                    # 'additional_special_tokens': ['[SP1]', '[SP2]']}\n",
        "    tokenizer.add_special_tokens(special_tokens)\n",
        "    vocab = tokenizer.get_vocab()\n",
        "    # print(vocab)\n",
        "    model.resize_token_embeddings(len(vocab))"
      ],
      "metadata": {
        "id": "pCsWDH4Z6VO2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### test\n",
        "tokenizer(data['title'].iloc[0], return_tensors='pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNMrBKKF2GKJ",
        "outputId": "aaa58bb8-b9c0-46b7-bffa-53ad19b55a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[18102,  4734,   376,  1186,  7994, 40983,  5334,  3893,  7276, 28871,\n",
              "           532,   383,   968,  1971,  3782]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, max_length=1024, use_title=False):\n",
        "        super().__init__()\n",
        "\n",
        "        ### preprocessing\n",
        "        data = pd.read_csv(data_path, index_col=0)[['content']]\n",
        "        safe_len = data['content'].apply(lambda x: len(x.split(' ')) < 350)\n",
        "        clean = []\n",
        "        for line in data['content']:\n",
        "            if len(line.split(' ')) > 350:\n",
        "                truncated_line = ' '.join(line.split(' ')[:350])\n",
        "                clean.append(truncated_line.lower())\n",
        "        clean = pd.DataFrame(clean, columns=['content'])\n",
        "        self.data = pd.concat([data[safe_len], clean], axis=0)        \n",
        "        ### ### ### ### ### \n",
        "        self.tokenizer = tokenizer\n",
        "        self.use_title = use_title\n",
        "    def __getitem__(self, index):\n",
        "        if self.use_title and random.random() < 0.01:\n",
        "            text = self.data['title'].iloc[index]\n",
        "        text = self.data['content'].iloc[index]\n",
        "        # truncation 하면 전처리 대체 가능?\n",
        "        inputs = self.tokenizer(f'<|endoftext|>{text}<|endoftext|>', return_tensors='pt')\n",
        "        # inputs['labels'] = inputs['input_ids'][index]\n",
        "        ids_len, mask_len = inputs['input_ids'].size(1), inputs['attention_mask'].size(1)\n",
        "        return inputs, ids_len, mask_len\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "rC3kUWI7PMAg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NewsCollate(batch):\n",
        "    batch, ids_len, mask_len = zip(*batch)\n",
        "    max_ids, max_mask = max(ids_len), max(mask_len)\n",
        "    ids_res, mask_res, label_res = [], [], []\n",
        "    for i, sample in enumerate(batch):\n",
        "        ids_pad = max_ids - ids_len[i]\n",
        "        mask_pad = max_mask - mask_len[i]\n",
        "        \n",
        "        ids_tensor = torch.cat([sample['input_ids'], torch.LongTensor([[tokenizer.get_vocab()['<|endoftext|>']] * ids_pad])], dim=1)\n",
        "        if want_to_change_vocab:\n",
        "            ids_tensor = torch.cat([sample['input_ids'], torch.LongTensor([[tokenizer.get_vocab()['[PAD]']] * ids_pad])], dim=1)\n",
        "        mask_tensor = torch.cat([sample['attention_mask'], torch.LongTensor([[0] * mask_pad])], dim=1)\n",
        "        ids_res.append(ids_tensor)\n",
        "        mask_res.append(mask_tensor)\n",
        "        # label_res.append(sample['labels'].reshape(-1))\n",
        "    ids_batch = torch.cat(ids_res, dim=0)\n",
        "    mask_batch = torch.cat(mask_res, dim=0)\n",
        "    return {'input_ids':ids_batch, 'attention_mask':mask_batch}"
      ],
      "metadata": {
        "id": "eNlBTOer47us"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NewsDataset(args.train_path, tokenizer)\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.num_workers,\n",
        "                        collate_fn=NewsCollate,\n",
        "                        shuffle=args.shuffle)\n",
        "valid_dataset = NewsDataset(args.valid_path, tokenizer)\n",
        "valid_dataloader = DataLoader(dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.num_workers,\n",
        "                        collate_fn=NewsCollate)\n",
        "for i in dataloader:\n",
        "    print(i)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lC4dHC5zVDQ",
        "outputId": "1eac9d8a-c03b-480e-ec7c-b0ccc7ff77f9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[50256,  1640,   257,   614,   290,   257,  2063,    11,  6899, 38616,\n",
            "          1122,   290, 46412, 16225,   956,    11,   734,  1263,  3056,  2214,\n",
            "          2594,  2706,    11,   550,   587,  5670,   319,   511,   720,  2327,\n",
            "          2997, 24589,    13,   326, 24066,    11,   772,   355, 19464,  4536,\n",
            "         44525,   290,   511, 14495,  2005,  3484,   284,  7866,    11,   318,\n",
            "          3443,   625,    13,   262,   734,  2706,  3414,   287,   257,  2643,\n",
            "           319, 37437,   323,   326,   484,   550,  3066,   284, 23654,   511,\n",
            "         24589,    13,   262,  1705,  1625,   706,   281, 49069,   306,   890,\n",
            "         11344,  2423,  1429,   326, 45200,   287,   257,  8087,   938,  1227,\n",
            "           416,   262,  5316,  5011,   284,  2512,   262,  1730,   319, 42766,\n",
            "          9384,    13,   564,   250,  4514,  1111,  2706,  2938,   262,  5150,\n",
            "         24589,   284,  1255,   287, 13206,  4034,   284, 19195,    11,  4297,\n",
            "           290,   584, 26138,    11,  6459,   287, 16727,  5637, 11344, 45818,\n",
            "           290,  2276,  2831,  3403,   326, 15052,  9694,  1730, 12446,  2957,\n",
            "           284,   262,  7664,   326, 19883,   318,   262,  1266,  1781,   286,\n",
            "          2223,    11,   447,   251,   531,   288,  1015, 10287,   283,    11,\n",
            "          8900,   290,  4039,  4640,   286,  6899, 38616,  1122,    11,   287,\n",
            "         37437,   323,   447,   247,    82,  2643,    13,   262,  1730,   373,\n",
            "           530,   326,  4376, 26928,   422,   262,   923,   319,  1771,   340,\n",
            "           561,   651,  1613, 17199,    13,   262,   734,  2706,   547,  6095,\n",
            "           284,  4097,  1978,   284,  9320,   351,   262,  7832,   286,  5513,\n",
            "            75,  4494,  1362,    11,   290,    11,   287,   262, 14324,    11,\n",
            "           284, 28602, 13188,   287,  3484,  3519,   284,  4560,   290,  2267,\n",
            "           290,  2478,    13,  1243,  3421, 22188,  2582,   706,   262,  1730,\n",
            "           373,  4488,   287,   645,   303,  1916,  1946,    13,  3056,  4536,\n",
            "         30895,   284,   511,  9016,  2974,   287,   812,    11,   257, 10538,\n",
            "           326,   468,  5676,   262,  2104,  2831,    13,  4200,   286,  6798,\n",
            "            11,   543,   547,  3306,   284, 47316, 17199,   447,   247,  4786,\n",
            "           326,   262,  6087,   373,   655,  1165,  1263,    11,  6451,  2627,\n",
            "           517, 12827,    13,   220,   220, 14456,  2900, 29879,   284,  6687,\n",
            "           511,   898,  1597,  1141,   262, 34540,    11,   290,  2839, 13598,\n",
            "          9658,   319,   262, 29394,    13,   351,   262,  2706,   447,   247,\n",
            "         16612,   284, 22085,    11,   262,  5316,  5011, 16334,   287, 46593,\n",
            "           346,   284,  2512,   262,  1730,    11,  2282,   340,   561,   564,\n",
            "           250,   417,   320,  4559,  9204,  5449,    11, 43370,  2568,  5939,\n",
            "           290,  4419, 45630,   272,  7008,    13,   564,   251,   326,  1364,\n",
            "           262,  2706,   351,  1310,   481,   284,  1907,    13,   564,   250,\n",
            "         10919,  6899, 38616,  1122,   743,   423,   285,  7860, 49262,   994,\n",
            "           318,   262, 19440,   286,   262, 34540,   290,  3729,   262, 20788,\n",
            "           286,   262,  5011,   286,  5316,   284,  7059,   663, 12749,   287,\n",
            "           257,  1588,  6355, 24589,    11,   447,   251,   531, 23963,  1667,\n",
            "          1155,  8326,    11,   281, 12499,   379,  2239,  5135,   753,    13,\n",
            "           355,  9836,   329,   262, 41584,    11,  6899, 38616,  1122,  4987,\n",
            "           284,  1414, 46412, 16225,   956,   720,    18,    13, 50256, 50257,\n",
            "         50257, 50257],\n",
            "        [50256, 22540, 19997,   373,   991, 44337, 29445,  3462,  1755,    11,\n",
            "           355,   339, 10282,   656,  8073,   287,   262,  2330,  2156, 11566,\n",
            "           351,   465,  7705,   286,  1181,    11,   302,    87,   266,    13,\n",
            "         10597,   882,    11,   617,  1987,  2250,   706,  3501,   262,   749,\n",
            "         44777,  4046,   286,   465,  4506, 12112,    13,   475,   407,   890,\n",
            "         20875,    11,   262, 19634,   422,   285,    81,    13, 19997,   447,\n",
            "           247,    82,  1266,  1110,   287,  2607,  2540,   284, 22100,   351,\n",
            "           262,  7163,  1705,   326,   465,  6136,  2276,    11, 11223,   487,\n",
            "         10991,    11,   550,  1138,   351,   262,   374, 31562, 14791,  1141,\n",
            "           262,  1584,  1923,    13,   285,    81,    13, 10991,  4054,   284,\n",
            "          3068,   883, 10275,   287,   465, 34548, 12641,  4854,    11,   393,\n",
            "            11,  1864,   284,  4787, 20409,    11,   284,  1560,   285,    81,\n",
            "            13, 19997,   379,   477,    13,   262,  1621, 43861,   285,    81,\n",
            "            13, 19997,   447,   247,    82,  3187,   262,  1306,  1110,   284,\n",
            "           262,  6215, 11920,   308, 12573,   374,    13,   329,    67,    11,\n",
            "           257,  6833,  4787,  3663,   284,  7238,   465,  2597,   355, 11561,\n",
            "           287,  4039,    13,   290,   416,   262,   640,   339,  1392,   736,\n",
            "           284,   262,  2330,  2156,   319,   294,  3479,  1755,    11,   262,\n",
            "          1893,  1309,   465, 14285,   905,    13,   287,   257,  2643, 20394,\n",
            "           257,  5385, 19976,   326,  5084,  1381,   547,   319,   257,   564,\n",
            "           250, 42248, 12601,   447,   251,   625,   262,  3662,   447,   247,\n",
            "            82,  8470,   351,   374, 31269,    11,   285,    81,    13, 19997,\n",
            "          4438,   257,  6427,   475,  6235,  1171, 33896,   379,   703,   285,\n",
            "            81,    13, 10991,   550, 12118,   262,  2300,    13,   564,   250,\n",
            "           258,   714,   423,  5081,   465,  2882,   517, 14351,    11,   447,\n",
            "           251,   285,    81,    13, 19997,   531,    13,   329,   285,    81,\n",
            "            13, 19997,    11,   340,   373,   262,  3452, 49591, 22007, 12174,\n",
            "           683,   422, 13977, 23692,   706,   257, 15074, 13852,    88,   717,\n",
            "          1227,   287,  2607,   326,   468,   587,  7498,   416,  4858,  2260,\n",
            "          8536,    11,   262, 25693,   286,   465,  2260,  2324, 12534,    11,\n",
            "           290, 15074,  1877,  7546, 10109,    13,   262,  1893,   373, 38635,\n",
            "           326,   285,    81,    13, 10991,   750,   407,   517,  7773,  3280,\n",
            "           262,  2683,   339,   373,  1965,   739, 17865,    11,  1864,   284,\n",
            "           661,   508,  5158,   351,   683,    13,   465,  4025, 14285,    11,\n",
            "          2158,    11,   373,   407,   351,   285,    81,    13, 10991,    11,\n",
            "           475,   351, 16958,  4602,   262,  8292,   284,  7638,   329,   262,\n",
            "         20518,  1122,  1281,    13,   285,    81,    13, 19997,    11,  1864,\n",
            "           284,   465, 20409,  2641,   290,  2354,   286,   262,  2330,  2156,\n",
            "            11,   468,  2936, 41680,   416,   644,   339, 13957,   355,   257,\n",
            "          4632, 12524, 25250,    11, 17747,   287,   636,   286,  5084,  1381,\n",
            "           290,   661,   508,  6886,   465,  3071,   508,   389,   783, 30340,\n",
            "           465, 12112,   351, 17316,    13,   339,  5804,   326,   484,   389,\n",
            "          2157,   262,  3923,   546, 10802,   290, 19327,   287,   465,  3662,\n",
            "           290,    11,   749,   286,   477,    11,   326,   484,   423,   925,\n",
            "           465, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logger(name: str, file_path: str, stream=False):\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    \n",
        "    # 시간, 로거 이름, 로깅 레벨, 메세지\n",
        "    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
        "    # console에 출력하도록 설정\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    # 현재 디렉토리에 파일로 로깅하도록 설정\n",
        "    file_handler = logging.FileHandler(file_path)\n",
        "\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    if stream:\n",
        "        logger.addHandler(stream_handler)\n",
        "    # 현재 디렉토리에 로깅 저장\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "def save(filename, model, optimizer, logger):\n",
        "    state = {\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    torch.save(state, filename)\n",
        "    logger.info('Model saved')\n",
        "\n",
        "\n",
        "def load(filename, model, optimizer, logger):\n",
        "    # state = torch.load(filename)\n",
        "    state = torch.load(filename, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state['model'])\n",
        "    if 'optimizer' in state and optimizer:\n",
        "        optimizer.load_state_dict(state['optimizer'])\n",
        "    logger.info('Model loaded : {}'.format(filename))"
      ],
      "metadata": {
        "id": "wyRkVUtp8caB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer, dataloader, optimizer,\n",
        "          lr_scheduler, epoch, train_begin, device):\n",
        "    begin = epoch_begin = time.time()\n",
        "    print_batch = 100\n",
        "\n",
        "    total_num, total_batch_size = 0, len(dataloader)\n",
        "    losses, batch_cnt = 0, 0\n",
        "    print('train start...')\n",
        "    for batch in dataloader:\n",
        "        batch = {k:v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch, labels=batch['input_ids'])\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "\n",
        "        losses += loss.item()\n",
        "        total_num += 1\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch_cnt % print_batch == 0:\n",
        "            current = time.time()\n",
        "            elapsed = current - begin\n",
        "            epoch_elapsed = (current - epoch_begin) / 60.0\n",
        "            train_elapsed = (current - train_begin) / 3600.0\n",
        "            print('epoch: {:4d}, batch: {:5d}/{:5d}, lr: {:.16f},\\nloss: {:.8f}, elapsed: {:6.2f}s {:6.2f}m {:6.2f}h'.format(\n",
        "                epoch, batch_cnt, total_batch_size,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                losses / total_num,\n",
        "                elapsed, epoch_elapsed, train_elapsed))\n",
        "            begin = time.time()\n",
        "\n",
        "        batch_cnt += 1\n",
        "    print('train completed...')\n",
        "    return losses / total_batch_size"
      ],
      "metadata": {
        "id": "HeTCBsnCzjo3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer, dataloader, epoch, device):  \n",
        "    model.eval()\n",
        "    losses, batch_cnt = 0, 0\n",
        "    print('evaluate start...')\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if batch_cnt % 500 == 0:\n",
        "                batch = {k:v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch, labels=batch['input_ids'])\n",
        "                loss = outputs[0]\n",
        "                print(f\"loss {loss}\")\n",
        "                losses = loss.item()\n",
        "            batch_cnt += 1\n",
        "    print('train completed...')\n",
        "    return losses / len(dataloader)"
      ],
      "metadata": {
        "id": "N-4LBEqgyt6k"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "num_training_steps = args.epochs * len(dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=8000,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "logger = get_logger(name='train',\n",
        "                    file_path=os.path.join('.', 'train_log.log'),\n",
        "                    stream=True)\n",
        "\n",
        "train_begin = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train_loss = train(model, tokenizer, dataloader, optimizer, lr_scheduler, epoch, train_begin, args.device)\n",
        "    logger.info('Epoch %d (Training) Loss %0.8f' % (epoch, train_loss))\n",
        "    \n",
        "    valid_loss = evaluate(model, tokenizer, valid_dataloader, epoch, args.device)\n",
        "    \n",
        "    model.eval()\n",
        "    inputs = tokenizer.encode('''How serious is the presence of the Covid virus in deer for humans?\"''', return_tensors='pt')\n",
        "    sample_outputs = model.generate(\n",
        "                            bos_token_id=inputs,\n",
        "                            do_sample=True,   \n",
        "                            top_k=50, \n",
        "                            max_length = 100,\n",
        "                            top_p=0.90, \n",
        "                            num_return_sequences=3\n",
        "                        )\n",
        "\n",
        "    print(\"Output:\\n\" + 100 * '-')\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "        print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "ljNiAiZgiRDi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "93d9af90-f726-494c-984b-8382cde71ff7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train start...\n",
            "epoch:    0, batch:     0/24964, lr: 0.0000000875000000,\n",
            "loss: 7.80753326, elapsed:  17.66s   0.29m   0.00h\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-75d7876acfd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_begin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %d (Training) Loss %0.8f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-ef80d27e6b7a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, dataloader, optimizer, lr_scheduler, epoch, train_begin, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J5CkqZCt2GEK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}